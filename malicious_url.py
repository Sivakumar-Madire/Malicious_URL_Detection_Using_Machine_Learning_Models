# -*- coding: utf-8 -*-
"""Malicious_URL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15NT9IcZb5mk2LPfqMjYlYGpKNOq5zwZ5
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn import metrics
import warnings
warnings.filterwarnings('ignore')

data0 = pd.read_csv("phishing.csv")

data0

#Checking the shape of the dataset
data0.shape



#Listing the features of the dataset
data0.columns

data0.info()

# nunique value in columns

data0.nunique()

#droping index column

data0 = data0.drop(['Index'],axis = 1)

#description of dataset

data0.describe().T

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

# Sample data for demonstration
labels = ['Phishing', 'Legitimate']
sizes = [6157, 4897]

# Creating the 3D plot
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.bar3d(1, 1, 0, 0.5, 0.5, sizes[0], shade=True, color='r', alpha=0.8)
ax.bar3d(2, 2, 0, 0.5, 0.5, sizes[1], shade=True, color='g', alpha=0.8)

# Setting labels and title
ax.set_xlabel('X Label')
ax.set_ylabel('Y Label')
ax.set_zlabel('Z Label')
ax.set_title('Distribution of Phishing Instances')

# Show the plot
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Count the occurrences of each online transaction type
transaction_type_counts = data0['class'].value_counts()

# Create a bar plot
plt.figure(figsize=(10, 5))
ax = sns.barplot(x=transaction_type_counts.index, y=transaction_type_counts, palette="Blues")
plt.title("Number of Rows for Each Class")
plt.xlabel("Type of Class")
plt.ylabel("Number of Rows")
plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility

# Add exact numbers on top of the bars
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='baseline', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')

plt.show()

#Correlation heatmap
plt.figure(figsize=(15,15))
sns.heatmap(data0.corr(), annot=True)
plt.show()

#pairplot for particular features

df = data0[['PrefixSuffix-', 'SubDomains', 'HTTPS','AnchorURL','WebsiteTraffic','class']]
sns.pairplot(data = df,hue="class",corner=True);

# Phishing Count in pie chart

data0['class'].value_counts().plot(kind='pie',autopct='%1.2f%%')
plt.title("Phishing Count")
plt.show()

# Splitting the dataset into dependant and independant fetature

X = data0.drop(["class"],axis =1)
y = data0["class"]

# Replace this line in your code where you calculate the correlation matrix
corr_matrix = data0.corr()

# With this line
corr_matrix = data0.corr(numeric_only=True)

#Correlation heatmap
plt.figure(figsize=(15,13))
color = plt.get_cmap('viridis').copy()   # default color
color.set_bad('lightblue')
sns.heatmap(corr_matrix, annot=True, linewidth=0.4,cmap=color)
plt.show()

corr_matrix.shape

corr_matrix['class']

status_corr = corr_matrix['class']
status_corr.shape

X = data0.drop(["class"],axis =1)
y = data0["class"]

# Splitting the dataset into train and test sets: 80-20 split

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
X_train.shape, y_train.shape, X_test.shape, y_test.shape

# Creating holders to store the model performance results
ML_Model = []
accuracy = []
f1_score = []
recall = []
precision = []

#function to call for storing the results
def storeResults(model, a,b,c,d):
  ML_Model.append(model)
  accuracy.append(round(a, 3))
  f1_score.append(round(b, 3))
  recall.append(round(c, 3))
  precision.append(round(d, 3))



# Splitting the dataset into train and test sets: 80-20 split
from sklearn.model_selection import train_test_split

X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size = 0.2, random_state = 12)

# check the shape of X_train, X_test, y_train and y_test
print('X_train shape: ', X_train1.shape)
print('X_test shape: ', X_test1.shape)
print('y_train shape: ', y_train1.shape)
print('y_test shape: ', y_test1.shape)

from sklearn.tree import DecisionTreeClassifier
# Instantiate the model
DT_model = DecisionTreeClassifier(max_depth=3)

# Fit the model to the training set
DT_model.fit(X_train1, y_train1)

y_pred = DT_model.predict(X_test1)

from sklearn.metrics import accuracy_score

# Assuming y_pred is your model's predictions on the test set
print('Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test1, y_pred)))

# print the scores on training and test set
print('Training set score: {:.4f}'.format(DT_model.score(X_train1, y_train1)))
print('Test set score: {:.4f}'.format(DT_model.score(X_test1, y_test1)))

from sklearn.model_selection import KFold

# Instantiate KFold with the desired number of splits
kf = KFold(n_splits=10, shuffle=False)

from sklearn.model_selection import cross_val_score

# Assuming DT_model is your Decision Tree model
score = cross_val_score(DT_model, X_train1, y_train1, cv=kf, scoring='accuracy')
DT_model_cv_score = score.mean()
DT_model_cv_stdev = score.std()
print('Cross Validation Accuracy scores are:\n {}'.format(score))

Accuracy  = ['Cross Validation Accuracy ']
DT_A = pd.DataFrame({'CV Mean':DT_model_cv_score,'Std':DT_model_cv_stdev},index=Accuracy )
DT_A

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(DT_model, X_test1, y_test1, colorbar=False, cmap='Greens')
plt.title('Confusion Matrix of Base Decision Tree')
plt.grid(False)

from sklearn.metrics import classification_report, precision_score, recall_score, f1_score

# Assuming y_pred is your predictions on the test set
print(classification_report(y_test1, y_pred))

def metrics_calculator(y_test, y_pred, model_name):
    '''
    This function calculates all desired performance metrics for a given model.
    '''
    result = pd.DataFrame(data=[accuracy_score(y_test, y_pred),
                                precision_score(y_test, y_pred, average='macro'),
                                recall_score(y_test, y_pred, average='macro'),
                                f1_score(y_test, y_pred, average='macro')],
                          index=['Accuracy','Precision','Recall','F1-score'],
                          columns = [model_name])
    return result

BaseDT_result = metrics_calculator(y_test1, y_pred, 'Base Decision Tree')
BaseDT_result

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

y_Pred_prob = DT_model.predict_proba(X_test1)

def roc_curve_plot(y_actual, y_predicted_probs, figsize=(5, 4), title=None, legend_loc='best'):

    # Compute ROC curve and ROC area for each class
    fpr = {}
    tpr = {}
    thres = {}
    roc_auc = {}

    n_class = y_predicted_probs.shape[1]
    for i in range(n_class):
        fpr[i], tpr[i], thres[i] = roc_curve(y_actual == i, y_predicted_probs[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Create a figure and plot the ROC curve for each class
    plt.figure(figsize=figsize)
    for i in range(n_class):
        plt.plot(fpr[i], tpr[i], linewidth=1, label='Class {}: AUC={:.2f}'.format(i, roc_auc[i]))

    # Add diagonal line and axis labels
    plt.plot([0, 1], [0, 1], '--', linewidth=0.5)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')

    # Set axis limits and add title and legend
    plt.xlim([0, 1])
    plt.ylim([0, 1.05])
    if title is not None:
        plt.title(title)
    plt.legend(loc=legend_loc)

    # Show the plot
    plt.show()

roc_curve_plot(y_test1, y_Pred_prob)

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Assuming DT_model is your Decision Tree model
feature_names = list(X.columns)
target_names = ["0", "1", "2", "3"]

# Create a figure and plot the decision tree
plt.figure(figsize=(20, 10))
plot_tree(DT_model, filled=True, feature_names=feature_names, class_names=target_names, rounded=True, fontsize=10)

# Display the plot
plt.show()

# Get feature importances from the Random Forest model
importances = DT_model.feature_importances_

# Sort the features by importance
indices = np.argsort(importances)

# Create a horizontal bar plot of the feature importances
plt.figure(figsize=(6, 5))
plt.barh(range(len(indices)), importances[indices], color='#559d96', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')

# Add labels to the bars
for i, v in enumerate(importances[indices]):
    plt.text(v, i, str(round(v, 3)), color='black', fontsize= 7)

plt.show()

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Finding optimal hyperparameters(GridSearchCV)

# Define model
model = DecisionTreeClassifier()

# Define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)

# Define search parameters
max_depth = range(2, 19, 1)
criterion = ['entropy', 'gini']

#min_samples_split = [2, 3, 4]
#min_samples_leaf = [1, 2, 3]

param_grid = {'max_depth': max_depth, 'criterion':criterion}

# Define search
search = GridSearchCV(model, param_grid, scoring='accuracy', n_jobs=-1, cv=cv)

# Execute search
GridSearchCV = search.fit(X_train1, y_train1)

# Set the clf to the best combination of parameters
DT_modelcv = GridSearchCV.best_estimator_

# Summarize result
print('Best Score: %s' % GridSearchCV.best_score_)
print('Best Hyperparameters: %s' % GridSearchCV.best_params_)

DT_modelcv.fit(X_train1, y_train1)

y_pred = DT_modelcv.predict(X_test1)

print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test1, y_pred)))

ConfusionMatrixDisplay.from_estimator(DT_modelcv, X_test1, y_test1, colorbar=False, cmap='Greens')
plt.title('Confusion Matrix of Tuned Decision Tree')
plt.grid(False)

print(classification_report(y_test1, y_pred))

TunedDT_result = metrics_calculator(y_test1, y_pred, 'Tuned Decision Tree')
TunedDT_result

y_Pred_prob = DT_modelcv.predict_proba(X_test1)

def roc_curve_plot(y_actual, y_predicted_probs, figsize=(5, 4), title=None, legend_loc='best'):

    # Compute ROC curve and ROC area for each class
    fpr = {}
    tpr = {}
    thres = {}
    roc_auc = {}

    n_class = y_predicted_probs.shape[1]
    for i in range(n_class):
        fpr[i], tpr[i], thres[i] = roc_curve(y_actual == i, y_predicted_probs[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Create a figure and plot the ROC curve for each class
    plt.figure(figsize=figsize)
    for i in range(n_class):
        plt.plot(fpr[i], tpr[i], linewidth=1, label='Class {}: AUC={:.2f}'.format(i, roc_auc[i]))

    # Add diagonal line and axis labels
    plt.plot([0, 1], [0, 1], '--', linewidth=0.5)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')

    # Set axis limits and add title and legend
    plt.xlim([0, 1])
    plt.ylim([0, 1.05])
    if title is not None:
        plt.title(title)
    plt.legend(loc=legend_loc)

    # Show the plot
    plt.show()

roc_curve_plot(y_test1, y_Pred_prob)

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Assuming DT_modelcv is your Decision Tree model obtained from GridSearchCV
feature_names = list(X.columns)
target_names = ["0", "1", "2", "3"]

# Create a figure and plot the decision tree
plt.figure(figsize=(30, 15))
plot_tree(DT_modelcv, filled=True, feature_names=feature_names, class_names=target_names, rounded=True, fontsize=10)

# Display the plot
plt.show()

# Get feature importances from the Random Forest model
importances = DT_modelcv.feature_importances_

# Sort the features by importance
indices = np.argsort(importances)

# Create a horizontal bar plot of the feature importances
plt.figure(figsize=(6, 5))
plt.barh(range(len(indices)), importances[indices], color='#559d96', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')

# Add labels to the bars
for i, v in enumerate(importances[indices]):
    plt.text(v, i, str(round(v, 3)), color='black', fontsize= 7)

plt.show()

# Create a decision tree classifier
clf = DecisionTreeClassifier()

# Generate the cost complexity pruning path for the decision tree using the training data
path = clf.cost_complexity_pruning_path(X_train1, y_train1)

# Extract the list of alphas and impurities from the pruning path
ccp_alphas = path.ccp_alphas
impurities = path.impurities

# Create a figure and axis object for the plot
fig, ax = plt.subplots(figsize=(10,5))

# Plot the total impurity vs. effective alpha using a step plot
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', color='#559d96', drawstyle="steps-post")

# Set the x and y labels and title for the plot
ax.set_xlabel("Effective alpha")
ax.set_ylabel("Total impurity of leaves")
ax.set_title("Total Impurity vs. Effective alpha for training set")

# Show the plot
plt.show()

from sklearn import metrics
# Initialize empty lists to store classifier objects and evaluation metrics
clfs = []
accuracy_train = []
accuracy_test = []
recall_train = []
recall_test = []

# Loop through the values of ccp_alpha
for ccp_alpha in ccp_alphas:
    # Create a decision tree classifier object with the given value of ccp_alpha
    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)

    # Fit the classifier to the training data
    clf.fit(X_train1, y_train1)

    # Generate predictions for the training and test sets
    y_train_pred = clf.predict(X_train1)
    y_test_pred = clf.predict(X_test1)

    # Compute and store the accuracy scores for the training and test sets
    accuracy_train.append(clf.score(X_train1, y_train1))
    accuracy_test.append(clf.score(X_test1, y_test1))

    # Compute and store the recall scores for the training and test sets
    recall_train.append(metrics.recall_score(y_train1, y_train_pred, average='micro'))
    recall_test.append(metrics.recall_score(y_test1, y_test_pred, average='micro'))

    # Store the classifier object in the list of classifiers
    clfs.append(clf)

fig, ax = plt.subplots(figsize=(10, 5))

# Set the x-axis and y-axis label to "alpha" and "Recall"
ax.set_xlabel("alpha")
ax.set_ylabel("Recall")

# Set the title of the plot
ax.set_title("Recall vs. alpha for training and testing sets")

# Plot the recall scores for the training and test set as a function of ccp_alpha
ax.plot(ccp_alphas, recall_train, marker='o', label="train", drawstyle="steps-post", color='#559d96')
ax.plot(ccp_alphas, recall_test, marker='o', label="test", drawstyle="steps-post", color='#db735e')

# legend
ax.legend()
plt.show()

DT_Pruning = DecisionTreeClassifier(ccp_alpha= 0.0027, criterion= 'entropy', max_depth= 10)
DT_Pruning.fit(X_train1, y_train1)

y_pred = DT_Pruning.predict(X_test1)

ConfusionMatrixDisplay.from_estimator(DT_Pruning, X_test1, y_test1, colorbar=False, cmap='Greens')
plt.title('Confusion Matrix of Pruning Decision Tree')
plt.grid(False)

DT_Pruning_result = metrics_calculator(y_test1, y_pred, 'Post-pruning Decision Tree')
DT_Pruning_result

y_Pred_prob = DT_Pruning.predict_proba(X_test1)

def roc_curve_plot(y_actual, y_predicted_probs, figsize=(5, 4), title=None, legend_loc='best'):

    # Compute ROC curve and ROC area for each class
    fpr = {}
    tpr = {}
    thres = {}
    roc_auc = {}

    n_class = y_predicted_probs.shape[1]
    for i in range(n_class):
        fpr[i], tpr[i], thres[i] = roc_curve(y_actual == i, y_predicted_probs[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Create a figure and plot the ROC curve for each class
    plt.figure(figsize=figsize)
    for i in range(n_class):
        plt.plot(fpr[i], tpr[i], linewidth=1, label='Class {}: AUC={:.2f}'.format(i, roc_auc[i]))

    # Add diagonal line and axis labels
    plt.plot([0, 1], [0, 1], '--', linewidth=0.5)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')

    # Set axis limits and add title and legend
    plt.xlim([0, 1])
    plt.ylim([0, 1.05])
    if title is not None:
        plt.title(title)
    plt.legend(loc=legend_loc)

    # Show the plot
    plt.show()

roc_curve_plot(y_test1, y_Pred_prob)

# Get feature importances from the Random Forest model
importances = DT_Pruning.feature_importances_

# Sort the features by importance
indices = np.argsort(importances)

# Create a horizontal bar plot of the feature importances
plt.figure(figsize=(6, 5))
plt.barh(range(len(indices)), importances[indices], color='#559d96', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')

# Add labels to the bars
for i, v in enumerate(importances[indices]):
    plt.text(v, i, str(round(v, 3)), color='black', fontsize= 7)

plt.show()

Conclusion = pd.concat([BaseDT_result, TunedDT_result, DT_Pruning_result], axis=1)
Conclusion

# Create subplots with 2 rows and 4 columns
fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(16,8))

# Plot confusion matrix for Base Decision Tree
ax = axs[0, 0]
disp = ConfusionMatrixDisplay.from_estimator(DT_model, X_test1, y_test1, colorbar=False, cmap='Greens', ax=ax)
disp.ax_.set_title('Confusion Matrix of Base Decision Tree')
disp.ax_.grid(False)

# Plot confusion matrix for Tuned Decision Tree
ax = axs[0, 1]
disp = ConfusionMatrixDisplay.from_estimator(DT_modelcv, X_test1, y_test1, colorbar=False, cmap='Greens', ax=ax)
disp.ax_.set_title('Confusion Matrix of Tuned Decision Tree')
disp.ax_.grid(False)

# Plot confusion matrix for Pruning Decision Tree
ax = axs[0, 2]
disp = ConfusionMatrixDisplay.from_estimator(DT_Pruning, X_test1, y_test1, colorbar=False, cmap='Greens', ax=ax)
disp.ax_.set_title('Confusion Matrix of Pruning Decision Tree')
disp.ax_.grid(False)

# Hide the last subplot
axs[0,3].axis('off')

# Adjust spacing between subplots
plt.subplots_adjust(hspace=0.4, wspace=0.3)

plt.show()

def plot_confusion_matrix(test_Y, predict_y):
 C = confusion_matrix(test_Y, predict_y)
 A = (((C.T)/(C.sum(axis=1))).T)
 B = (C/C.sum(axis=0))

 plt.figure(figsize=(20,4))
 labels = [1,2]
 cmap=sns.light_palette("green")

 plt.subplot(1, 3, 1)
 sns.heatmap(confusion_matrix(test_Y, predict_y), annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)
 plt.xlabel('Predicted Class')
 plt.ylabel('Original Class')
 plt.title("Confusion matrix")

 plt.subplot(1, 3, 2)
 sns.heatmap(B, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)
 plt.xlabel('Predicted Class')
 plt.ylabel('Original Class')
 plt.title("Precision matrix")

 plt.subplot(1, 3, 3)
 sns.heatmap(A, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)
 plt.xlabel('Predicted Class')
 plt.ylabel('Original Class')
 plt.title("Recall matrix")

 plt.show()

Y = df['class']
Y = pd.DataFrame(Y)
Y.head()

train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.3, random_state=42)

print("train_X : ", train_X.shape)
print("test_X : ", test_X.shape)
print("train_Y : ", train_Y.shape)
print("test_Y : ", test_Y.shape)

dtree = DecisionTreeClassifier()
model = dtree.fit(train_X,train_Y)

train_tree = model.predict(train_X)
test_tree = model.predict(test_X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 12)

print('X_train shape: ', X_train.shape)
print('X_test shape: ', X_test.shape)
print('y_train shape: ', y_train.shape)
print('y_test shape: ', y_test.shape)

from sklearn.preprocessing import StandardScaler
# Scale the features using StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data and transform it
X_train = scaler.fit_transform(X_train)

# Transform the test data using the same scaler
X_test = scaler.transform(X_test)

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import GridSearchCV

# Define the base estimator with the desired criterion
base_estimator = DecisionTreeClassifier(criterion='gini')  # Change 'gini' to your desired criterion

# Define the BaggingClassifier with the base estimator
bagging_model = BaggingClassifier(estimator=base_estimator, random_state=0)

# Define the parameter grid for the grid search
param_grid = {
    'n_estimators': [10, 50, 100],
    'max_samples': [0.5, 1.0],
    'max_features': [0.5, 1.0],
}

# Perform grid search
grid_search = GridSearchCV(bagging_model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best BaggingClassifier
best_bagging_model = grid_search.best_estimator_

from sklearn.model_selection import cross_val_score
# Fit the best BaggingClassifier model on the training set
best_bagging_model.fit(X_train, y_train)

# Make predictions on the test set
predictions3 = best_bagging_model.predict(X_test)

# Now you can proceed with evaluating the model or performing other tasks

accuracy_bag = metrics.accuracy_score(predictions3, y_test)
print('Accuracy of the Bagged Decision Tree Model on the test set:', accuracy_bag)

# calculate and print the mean cross-validated score of the bagging model
cross_val_scores = cross_val_score(best_bagging_model, X_train, y_train, cv=10, scoring='accuracy')
print('Cross-validated Score of the Bagged Decision Tree Model:', cross_val_scores.mean())

Bagged_DecisionTree_result = metrics_calculator(y_test, predictions3, 'Tuned Bagged Decision Tree')
Bagged_DecisionTree_result

# Tuned Voting Classifier
y_pred_tuned_ensemble = best_bagging_model.predict(X_test)

# Tuned Voting Classifier
ConfusionMatrixDisplay.from_estimator(best_bagging_model, X_test, y_test, colorbar=False, cmap='Greens')
plt.title('Confusion Matrix of Best Ensembled Model')
plt.grid(False)

# Tuned Voting Classifier
print(classification_report(y_test, y_pred_tuned_ensemble))

# Tuned Voting Classifier
y_Pred_prob = best_bagging_model.predict_proba(X_test)

def roc_curve_plot(y_actual, y_predicted_probs, figsize=(5, 4), title=None, legend_loc='best'):

    # Compute ROC curve and ROC area for each class
    fpr = {}
    tpr = {}
    thres = {}
    roc_auc = {}

    n_class = y_predicted_probs.shape[1]
    for i in range(n_class):
        fpr[i], tpr[i], thres[i] = roc_curve(y_actual == i, y_predicted_probs[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Create a figure and plot the ROC curve for each class
    plt.figure(figsize=figsize)
    for i in range(n_class):
        plt.plot(fpr[i], tpr[i], linewidth=1, label='Class {}: AUC={:.2f}'.format(i, roc_auc[i]))

    # Add diagonal line and axis labels
    plt.plot([0, 1], [0, 1], '--', linewidth=0.5)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')

    # Set axis limits and add title and legend
    plt.xlim([0, 1])
    plt.ylim([0, 1.05])
    if title is not None:
        plt.title(title)
    plt.legend(loc=legend_loc)

    # Show the plot
    plt.show()

roc_curve_plot(y_test, y_Pred_prob)

import pickle
filename = 'model.pkl'
pickle.dump(best_bagging_model, open(filename,'wb'))

load_model=pickle.load(open(filename, 'rb'))
load_model

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Assuming you have your data stored in X and y variables
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the base estimator
base_estimator = LogisticRegression()

# Define the BaggingClassifier with the base estimator
bagging_model = BaggingClassifier(base_estimator, random_state=0)

# Fit the bagging model on the training set
bagging_model.fit(X_train, y_train)

# Make predictions on the test set
predictions = bagging_model.predict(X_test)

# Calculate accuracy of the Bagged Logistic Regression Model on the test set
accuracy_LR = accuracy_score(y_test, predictions)
print('Accuracy of the Bagged Logistic Regression Model on the test set:', accuracy_LR)

# Print classification report
print(classification_report(y_test, predictions))

# Calculate predicted probabilities for ROC curve
y_pred_prob = bagging_model.predict_proba(X_test)

# Plot ROC curve
def roc_curve_plot(y_actual, y_predicted_probs, figsize=(5, 4), title=None, legend_loc='best'):
    fpr, tpr, _ = roc_curve(y_actual, y_predicted_probs[:, 1])
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=figsize)
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc=legend_loc)
    plt.show()

roc_curve_plot(y_test, y_pred_prob)

# Linear regression model
from sklearn.linear_model import LogisticRegression
#from sklearn.pipeline import Pipeline

# instantiate the model
log = LogisticRegression()

# fit the model
log.fit(X_train,y_train)
#predicting the target value from the model for the samples

y_train_log = log.predict(X_train)
y_test_log = log.predict(X_test)

#computing the accuracy, f1_score, Recall, precision of the model performance

acc_train_log = metrics.accuracy_score(y_train,y_train_log)
acc_test_log = metrics.accuracy_score(y_test,y_test_log)
print("Logistic Regression : Accuracy on training Data: {:.3f}".format(acc_train_log))
print("Logistic Regression : Accuracy on test Data: {:.3f}".format(acc_test_log))
print()

f1_score_train_log = metrics.f1_score(y_train,y_train_log)
f1_score_test_log = metrics.f1_score(y_test,y_test_log)
print("Logistic Regression : f1_score on training Data: {:.3f}".format(f1_score_train_log))
print("Logistic Regression : f1_score on test Data: {:.3f}".format(f1_score_test_log))
print()

recall_score_train_log = metrics.recall_score(y_train,y_train_log)
recall_score_test_log = metrics.recall_score(y_test,y_test_log)
print("Logistic Regression : Recall on training Data: {:.3f}".format(recall_score_train_log))
print("Logistic Regression : Recall on test Data: {:.3f}".format(recall_score_test_log))
print()

precision_score_train_log = metrics.precision_score(y_train,y_train_log)
precision_score_test_log = metrics.precision_score(y_test,y_test_log)
print("Logistic Regression : precision on training Data: {:.3f}".format(precision_score_train_log))
print("Logistic Regression : precision on test Data: {:.3f}".format(precision_score_test_log))
#computing the classification report of the model

print(metrics.classification_report(y_test, y_test_log))

from sklearn.metrics import ConfusionMatrixDisplay

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_test, predictions)

# Display confusion matrix heatmap
plt.figure(figsize=(8, 6))
ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=bagging_model.classes_).plot(cmap='Greens', ax=plt.gca())
plt.title('Confusion Matrix of Bagged Logistic Regression Model')
plt.show()

from sklearn.metrics import accuracy_score

# Make predictions on the training set
train_predictions = bagging_model.predict(X_train)

# Calculate training accuracy
train_accuracy = accuracy_score(y_train, train_predictions)
print('Training Accuracy:', train_accuracy)

# Calculate test accuracy
test_accuracy = accuracy_score(y_test, predictions)
print('Test Accuracy:', test_accuracy)

# Ensure X_train, y_train, X_test, and y_test have compatible dimensions
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

from sklearn.naive_bayes import MultinomialNB
# Define the base estimator with the desired criterion
base1_estimator = MultinomialNB()  # Change 'gini' to your desired criterion

# Define the BaggingClassifier with the base estimator
bagging1_model = MultinomialNB()

# Define the parameter grid for the grid search
param_grid = {
    'n_estimators': [10, 50, 100],
    'max_samples': [0.5, 1.0],
    'max_features': [0.5, 1.0],
}

from sklearn.model_selection import cross_val_score
# Make predictions on the test set
predictions3 = best_bagging_model.predict(X_test)
best_bagging_model.fit(X_train, y_train)
cross_val_scores = cross_val_score(best_bagging_model, X_train, y_train, cv=10, scoring='accuracy')
print('Cross-validated Score of the Bagged MultinomialNB  Model:', cross_val_scores.mean())

from sklearn.ensemble import IsolationForest
# Define the base estimator with the desired criterion
base2_estimator = IsolationForest()  # Change 'gini' to your desired criterion

# Define the BaggingClassifier with the base estimator
bagging2_model = IsolationForest()

# Define the parameter grid for the grid search
param_grid = {
    'n_estimators': [10, 50, 100],
    'max_samples': [0.5, 1.0],
    'max_features': [0.5, 1.0],
}

from sklearn.model_selection import cross_val_score
# Make predictions on the test set
predictions3 = best_bagging_model.predict(X_test)
best_bagging_model.fit(X_train, y_train)
cross_val_scores = cross_val_score(best_bagging_model, X_train, y_train, cv=10, scoring='accuracy')
print('Cross-validated Score of the  IsolationForest  Model:', cross_val_scores.mean())

from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import MinMaxScaler

# Preprocess your data to ensure non-negative values
# Example: Scale the features to a non-negative range using Min-Max scaling
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train1)
X_test_scaled = scaler.transform(X_test1)

# Instantiate the Multinomial Naive Bayes model
NB_model = MultinomialNB()

# Fit the model to the training set
NB_model.fit(X_train_scaled, y_train1)

# Make predictions on the test set
y_pred_NB = NB_model.predict(X_test_scaled)

# Calculate the accuracy score
accuracy_NB = accuracy_score(y_test1, y_pred_NB)
print('Model accuracy score: {0:0.4f}'.format(accuracy_NB))
# Print the scores on the training and test set
print('Training set score: {:.4f}'.format(NB_model.score(X_train_scaled, y_train1)))
print('Test set score: {:.4f}'.format(NB_model.score(X_test_scaled, y_test1)))

# Perform cross-validation
from sklearn.model_selection import cross_val_score, KFold

# Instantiate KFold with the desired number of splits
kf = KFold(n_splits=10, shuffle=False)

# Calculate cross-validation scores
score_NB = cross_val_score(NB_model, X_train_scaled, y_train1, cv=kf, scoring='accuracy')
NB_model_cv_score = score_NB.mean()
NB_model_cv_stdev = score_NB.std()
print('Cross Validation Accuracy scores are:\n {}'.format(score_NB))

# Create a DataFrame to display cross-validation results
Accuracy_NB = ['Cross Validation Accuracy']
NB_A = pd.DataFrame({'CV Mean': NB_model_cv_score, 'Std': NB_model_cv_stdev}, index=Accuracy_NB)
NB_A

# Display the confusion matrix
from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(NB_model, X_test_scaled, y_test1, colorbar=False, cmap='Greens')
plt.title('Confusion Matrix of Multinomial Naive Bayes')
plt.grid(False)

# Print the classification report
print(classification_report(y_test1, y_pred_NB))

from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Instantiate the Isolation Forest model
iso_forest_model = IsolationForest(random_state=42)

# Fit the model to your data
iso_forest_model.fit(X_train1)

# Predict outliers/anomalies
y_pred_outliers = iso_forest_model.predict(X_test1)

# Replace predicted labels (-1 for outliers, 1 for inliers) with 0 for anomalies and 1 for normal points
y_pred_outliers[y_pred_outliers == 1] = 0  # Inliers labeled as 1
y_pred_outliers[y_pred_outliers == -1] = 1  # Outliers labeled as 0

# Evaluate the model
accuracy_ISO = accuracy_score(y_test1, y_pred_outliers)
print('Model accuracy score: {0:0.4f}'.format(accuracy_ISO))
# Print the confusion matrix and classification report
print('\nClassification Report:\n', classification_report(y_test1, y_pred_outliers))
print('Confusion Matrix:\n', confusion_matrix(y_test1, y_pred_outliers))
# Display the confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(confusion_matrix(y_test1, y_pred_outliers), cmap='Greens')
plt.title('Confusion Matrix of Isolation Forest')
plt.colorbar()
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Display the confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

conf_matrix = confusion_matrix(y_test1,  iso_forest_model.predict(X_test1))
ConfusionMatrixDisplay(conf_matrix, display_labels=['0', '1']).plot(cmap='Greens', colorbar=False)
plt.title('Confusion Matrix of Isolation Forest')
plt.grid(False)

from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score

# Instantiate the Hierarchical Clustering model
# Adjust parameters as needed (e.g., number of clusters, linkage type)
# linkage='ward' is often used, but you can experiment with other options
hierarchical_model = AgglomerativeClustering(n_clusters=3, linkage='ward')

# Fit the model to your data
hierarchical_model.fit(X_train1)  # Assuming X_train1 contains your training data

# Obtain cluster labels
cluster_labels = hierarchical_model.labels_

# Perform evaluation (optional)
silhouette_avg = silhouette_score(X_train1, cluster_labels)
print("Average silhouette_score:", silhouette_avg)

from sklearn.mixture import GaussianMixture
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

# Instantiate the Gaussian Mixture Model
gmm_model = GaussianMixture(n_components=2)  # Adjust the number of components as needed

# Fit the model to the training set
gmm_model.fit(X_train_scaled)

# Make predictions on the test set
y_pred_gmm = gmm_model.predict(X_test_scaled)

# Calculate the accuracy score
accuracy_gmm = accuracy_score(y_test1, y_pred_gmm)
print('Model accuracy score: {0:0.4f}'.format(accuracy_gmm))
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Compute confusion matrix
cm = confusion_matrix(y_test1, y_pred_gmm)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False)
plt.title('Confusion Matrix of Gaussian Mixture Model')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()


# Print the classification report
print(classification_report(y_test1, y_pred_gmm))

from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA

# Preprocess your data to ensure non-negative values
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train1)
X_test_scaled = scaler.transform(X_test1)

# Perform Principal Component Analysis (PCA) for dimensionality reduction
n_components = 10  # Choose your desired number of components
pca = PCA(n_components=n_components)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Instantiate the Gaussian Naive Bayes model
NB_model = GaussianNB()

# Fit the model to the training set
NB_model.fit(X_train_pca, y_train1)

# Make predictions on the test set
y_pred_NB = NB_model.predict(X_test_pca)

# Calculate the accuracy score
accuracy_NB = accuracy_score(y_test1, y_pred_NB)
print('Model accuracy score: {0:0.4f}'.format(accuracy_NB))

# Print the scores on the training and test set
print('Training set score: {:.4f}'.format(NB_model.score(X_train_pca, y_train1)))
print('Test set score: {:.4f}'.format(NB_model.score(X_test_pca, y_test1)))

# Perform cross-validation
from sklearn.model_selection import cross_val_score, KFold

# Instantiate KFold with the desired number of splits
kf = KFold(n_splits=10, shuffle=False)

# Calculate cross-validation scores
score_NB = cross_val_score(NB_model, X_train_pca, y_train1, cv=kf, scoring='accuracy')
NB_model_cv_score = score_NB.mean()
NB_model_cv_stdev = score_NB.std()
print('Cross Validation Accuracy scores are:\n {}'.format(score_NB))

# Create a DataFrame to display cross-validation results
Accuracy_NB = ['Cross Validation Accuracy']
NB_A = pd.DataFrame({'CV Mean': NB_model_cv_score, 'Std': NB_model_cv_stdev}, index=Accuracy_NB)
NB_A

# Display the confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

conf_matrix = confusion_matrix(y_test1, y_pred_NB)
ConfusionMatrixDisplay(conf_matrix, display_labels=NB_model.classes_).plot(cmap='Greens', colorbar=False)
plt.title('Confusion Matrix of Gaussian Naive Bayes')
plt.grid(False)

# Print the classification report
from sklearn.metrics import classification_report

print(classification_report(y_test1, y_pred_NB))

from sklearn.cluster import MeanShift

# Instantiate the Mean Shift clustering model
mean_shift_model = MeanShift()

# Fit the model to the training set
mean_shift_model.fit(X_train1)  # Assuming X_train1 contains your training data

# Get cluster labels for the test set
cluster_labels = mean_shift_model.predict(X_test1)  # Assuming X_test1 contains your test data

# Perform any necessary evaluation or visualization of clustering results
# For example, you can examine the cluster centers and the number of clusters found
cluster_centers = mean_shift_model.cluster_centers_
n_clusters = len(cluster_centers)
print(f'Number of clusters: {n_clusters}')
print('Cluster centers:')
print(cluster_centers)

# -*- coding: utf-8 -*-
"""
Created on Wed Mar 20 10:40:34 2024

@author: OKOKPRO
"""

import matplotlib.pyplot as plt

# Define the models and their corresponding accuracy scores
models = [
    "Decision tree classifier",
    "Base Decision Tree",
    "Tuned Decision Tree",
    "Post-pruning Decision Tree",
    "Bagged Decision Tree",
    "Tuned Bagged Decision Tree",
    "Logistic Regression",
    "MultinomialNB",
    "IsolationForest",
    "AgglomerativeClustering",
    "GaussianMixture",
    "PCA",
    "MeanShift"
]
accuracy_scores = [
    0.9597796780598374,
    0.903211,
    0.960199,
    0.932610,
    0.9669832654907282,
    0.968792,
    0.9317051108095884,
    0.9690133445816397,
    0.2365,
    0.24118526298973442,
    0.1940,
    0.8915,
    1
]

# Plotting
plt.figure(figsize=(10, 6))
plt.barh(models, accuracy_scores, color='skyblue')
plt.xlabel('Accuracy Score')
plt.title('Model Comparison')
plt.xlim(0, 1)  # Limit the x-axis to 0-1 for accuracy scores
plt.gca().invert_yaxis()  # Invert y-axis to have the highest score at the top
plt.show()

results=pd.DataFrame({'Model':["Decision tree classifier",
    "Base Decision Tree",
    "Tuned Decision Tree",
    "Post-pruning Decision Tree",
    "Bagged Decision Tree",
    "Tuned Bagged Decision Tree",
    "Logistic Regression",
    "MultinomialNB",
    "IsolationForest",
    "AgglomerativeClustering",
    "GaussianMixture",
    "PCA",
    "MeanShift"],
                     'Accuracy Score':[0.9597796780598374,
    0.903211,
    0.960199,
    0.932610,
    0.9669832654907282,
    0.968792,
    0.9317051108095884,
    0.9690133445816397,
    0.2365,
    0.24118526298973442,
    0.1940,
    0.8915,
    1]})
result_df=results.sort_values(by='Accuracy Score', ascending=False)
result_df=result_df.set_index('Model')
result_df

!pip install python-whois

!pip install python-googlesearch

import ipaddress
import re
import urllib.request
from bs4 import BeautifulSoup
import socket
import requests
import google
import whois
from datetime import date, datetime
import time
from dateutil.parser import parse as date_parse
from urllib.parse import urlparse

class FeatureExtraction:
    features = []
    def __init__(self,url):
        self.features = []
        self.url = url
        self.domain = ""
        self.whois_response = ""
        self.urlparse = ""
        self.response = ""
        self.soup = ""

        try:
            self.response = requests.get(url)
            self.soup = BeautifulSoup(response.text, 'html.parser')
        except:
            pass

        try:
            self.urlparse = urlparse(url)
            self.domain = self.urlparse.netloc
        except:
            pass

        try:
            self.whois_response = whois.whois(self.domain)
        except:
            pass




        self.features.append(self.UsingIp())
        self.features.append(self.longUrl())
        self.features.append(self.shortUrl())
        self.features.append(self.symbol())
        self.features.append(self.redirecting())
        self.features.append(self.prefixSuffix())
        self.features.append(self.SubDomains())
        self.features.append(self.Hppts())
        self.features.append(self.DomainRegLen())
        self.features.append(self.Favicon())


        self.features.append(self.NonStdPort())
        self.features.append(self.HTTPSDomainURL())
        self.features.append(self.RequestURL())
        self.features.append(self.AnchorURL())
        self.features.append(self.LinksInScriptTags())
        self.features.append(self.ServerFormHandler())
        self.features.append(self.InfoEmail())
        self.features.append(self.AbnormalURL())
        self.features.append(self.WebsiteForwarding())
        self.features.append(self.StatusBarCust())

        self.features.append(self.DisableRightClick())
        self.features.append(self.UsingPopupWindow())
        self.features.append(self.IframeRedirection())
        self.features.append(self.AgeofDomain())
        self.features.append(self.DNSRecording())
        self.features.append(self.WebsiteTraffic())
        self.features.append(self.PageRank())
        self.features.append(self.GoogleIndex())
        self.features.append(self.LinksPointingToPage())
        self.features.append(self.StatsReport())


     # 1.UsingIp
    def UsingIp(self):
        try:
            ipaddress.ip_address(self.url)
            return -1
        except:
            return 1

    # 2.longUrl
    def longUrl(self):
        if len(self.url) < 54:
            return 1
        if len(self.url) >= 54 and len(self.url) <= 75:
            return 0
        return -1

    # 3.shortUrl
    def shortUrl(self):
        match = re.search('bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|'
                    'yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|'
                    'short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|'
                    'doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|'
                    'db\.tt|qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|'
                    'q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|'
                    'x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|tr\.im|link\.zip\.net', self.url)
        if match:
            return -1
        return 1

    # 4.Symbol@
    def symbol(self):
        if re.findall("@",self.url):
            return -1
        return 1

    # 5.Redirecting//
    def redirecting(self):
        if self.url.rfind('//')>6:
            return -1
        return 1

    # 6.prefixSuffix
    def prefixSuffix(self):
        try:
            match = re.findall('\-', self.domain)
            if match:
                return -1
            return 1
        except:
            return -1

    # 7.SubDomains
    def SubDomains(self):
        dot_count = len(re.findall("\.", self.url))
        if dot_count == 1:
            return 1
        elif dot_count == 2:
            return 0
        return -1

    # 8.HTTPS
    def Hppts(self):
        try:
            https = self.urlparse.scheme
            if 'https' in https:
                return 1
            return -1
        except:
            return 1

    # 9.DomainRegLen
    def DomainRegLen(self):
        try:
            expiration_date = self.whois_response.expiration_date
            creation_date = self.whois_response.creation_date
            try:
                if(len(expiration_date)):
                    expiration_date = expiration_date[0]
            except:
                pass
            try:
                if(len(creation_date)):
                    creation_date = creation_date[0]
            except:
                pass

            age = (expiration_date.year-creation_date.year)*12+ (expiration_date.month-creation_date.month)
            if age >=12:
                return 1
            return -1
        except:
            return -1

    # 10. Favicon
    def Favicon(self):
        try:
            for head in self.soup.find_all('head'):
                for head.link in self.soup.find_all('link', href=True):
                    dots = [x.start(0) for x in re.finditer('\.', head.link['href'])]
                    if self.url in head.link['href'] or len(dots) == 1 or domain in head.link['href']:
                        return 1
            return -1
        except:
            return -1

    # 11. NonStdPort
    def NonStdPort(self):
        try:
            port = self.domain.split(":")
            if len(port)>1:
                return -1
            return 1
        except:
            return -1

    # 12. HTTPSDomainURL
    def HTTPSDomainURL(self):
        try:
            if 'https' in self.domain:
                return -1
            return 1
        except:
            return -1

    # 13. RequestURL
    def RequestURL(self):
        try:
            for img in self.soup.find_all('img', src=True):
                dots = [x.start(0) for x in re.finditer('\.', img['src'])]
                if self.url in img['src'] or self.domain in img['src'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            for audio in self.soup.find_all('audio', src=True):
                dots = [x.start(0) for x in re.finditer('\.', audio['src'])]
                if self.url in audio['src'] or self.domain in audio['src'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            for embed in self.soup.find_all('embed', src=True):
                dots = [x.start(0) for x in re.finditer('\.', embed['src'])]
                if self.url in embed['src'] or self.domain in embed['src'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            for iframe in self.soup.find_all('iframe', src=True):
                dots = [x.start(0) for x in re.finditer('\.', iframe['src'])]
                if self.url in iframe['src'] or self.domain in iframe['src'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            try:
                percentage = success/float(i) * 100
                if percentage < 22.0:
                    return 1
                elif((percentage >= 22.0) and (percentage < 61.0)):
                    return 0
                else:
                    return -1
            except:
                return 0
        except:
            return -1

    # 14. AnchorURL
    def AnchorURL(self):
        try:
            i,unsafe = 0,0
            for a in self.soup.find_all('a', href=True):
                if "#" in a['href'] or "javascript" in a['href'].lower() or "mailto" in a['href'].lower() or not (url in a['href'] or self.domain in a['href']):
                    unsafe = unsafe + 1
                i = i + 1

            try:
                percentage = unsafe / float(i) * 100
                if percentage < 31.0:
                    return 1
                elif ((percentage >= 31.0) and (percentage < 67.0)):
                    return 0
                else:
                    return -1
            except:
                return -1

        except:
            return -1

    # 15. LinksInScriptTags
    def LinksInScriptTags(self):
        try:
            i,success = 0,0

            for link in self.soup.find_all('link', href=True):
                dots = [x.start(0) for x in re.finditer('\.', link['href'])]
                if self.url in link['href'] or self.domain in link['href'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            for script in self.soup.find_all('script', src=True):
                dots = [x.start(0) for x in re.finditer('\.', script['src'])]
                if self.url in script['src'] or self.domain in script['src'] or len(dots) == 1:
                    success = success + 1
                i = i+1

            try:
                percentage = success / float(i) * 100
                if percentage < 17.0:
                    return 1
                elif((percentage >= 17.0) and (percentage < 81.0)):
                    return 0
                else:
                    return -1
            except:
                return 0
        except:
            return -1

    # 16. ServerFormHandler
    def ServerFormHandler(self):
        try:
            if len(self.soup.find_all('form', action=True))==0:
                return 1
            else :
                for form in self.soup.find_all('form', action=True):
                    if form['action'] == "" or form['action'] == "about:blank":
                        return -1
                    elif self.url not in form['action'] and self.domain not in form['action']:
                        return 0
                    else:
                        return 1
        except:
            return -1

    # 17. InfoEmail
    def InfoEmail(self):
        try:
            if re.findall(r"[mail\(\)|mailto:?]", self.soap):
                return -1
            else:
                return 1
        except:
            return -1

    # 18. AbnormalURL
    def AbnormalURL(self):
        try:
            if self.response.text == self.whois_response:
                return 1
            else:
                return -1
        except:
            return -1

    # 19. WebsiteForwarding
    def WebsiteForwarding(self):
        try:
            if len(self.response.history) <= 1:
                return 1
            elif len(self.response.history) <= 4:
                return 0
            else:
                return -1
        except:
             return -1

    # 20. StatusBarCust
    def StatusBarCust(self):
        try:
            if re.findall("<script>.+onmouseover.+</script>", self.response.text):
                return 1
            else:
                return -1
        except:
             return -1

    # 21. DisableRightClick
    def DisableRightClick(self):
        try:
            if re.findall(r"event.button ?== ?2", self.response.text):
                return 1
            else:
                return -1
        except:
             return -1

    # 22. UsingPopupWindow
    def UsingPopupWindow(self):
        try:
            if re.findall(r"alert\(", self.response.text):
                return 1
            else:
                return -1
        except:
             return -1

    # 23. IframeRedirection
    def IframeRedirection(self):
        try:
            if re.findall(r"[<iframe>|<frameBorder>]", self.response.text):
                return 1
            else:
                return -1
        except:
             return -1

    # 24. AgeofDomain
    def AgeofDomain(self):
        try:
            creation_date = self.whois_response.creation_date
            try:
                if(len(creation_date)):
                    creation_date = creation_date[0]
            except:
                pass

            today  = date.today()
            age = (today.year-creation_date.year)*12+(today.month-creation_date.month)
            if age >=6:
                return 1
            return -1
        except:
            return -1

    # 25. DNSRecording
    def DNSRecording(self):
        try:
            creation_date = self.whois_response.creation_date
            try:
                if(len(creation_date)):
                    creation_date = creation_date[0]
            except:
                pass

            today  = date.today()
            age = (today.year-creation_date.year)*12+(today.month-creation_date.month)
            if age >=6:
                return 1
            return -1
        except:
            return -1

    # 26. WebsiteTraffic
    def WebsiteTraffic(self):
        try:
            rank = BeautifulSoup(urllib.request.urlopen("http://data.alexa.com/data?cli=10&dat=s&url=" + url).read(), "xml").find("REACH")['RANK']
            if (int(rank) < 100000):
                return 1
            return 0
        except :
            return -1

    # 27. PageRank
    def PageRank(self):
        try:
            prank_checker_response = requests.post("https://www.checkpagerank.net/index.php", {"name": self.domain})

            global_rank = int(re.findall(r"Global Rank: ([0-9]+)", rank_checker_response.text)[0])
            if global_rank > 0 and global_rank < 100000:
                return 1
            return -1
        except:
            return -1


    # 28. GoogleIndex
    def GoogleIndex(self):
        try:
            site = search(self.url, 5)
            if site:
                return 1
            else:
                return -1
        except:
            return 1

    # 29. LinksPointingToPage
    def LinksPointingToPage(self):
        try:
            number_of_links = len(re.findall(r"<a href=", self.response.text))
            if number_of_links == 0:
                return 1
            elif number_of_links <= 2:
                return 0
            else:
                return -1
        except:
            return -1

    # 30. StatsReport
    def StatsReport(self):
        try:
            url_match = re.search(
        'at\.ua|usa\.cc|baltazarpresentes\.com\.br|pe\.hu|esy\.es|hol\.es|sweddy\.com|myjino\.ru|96\.lt|ow\.ly', url)
            ip_address = socket.gethostbyname(self.domain)
            ip_match = re.search('146\.112\.61\.108|213\.174\.157\.151|121\.50\.168\.88|192\.185\.217\.116|78\.46\.211\.158|181\.174\.165\.13|46\.242\.145\.103|121\.50\.168\.40|83\.125\.22\.219|46\.242\.145\.98|'
                                '107\.151\.148\.44|107\.151\.148\.107|64\.70\.19\.203|199\.184\.144\.27|107\.151\.148\.108|107\.151\.148\.109|119\.28\.52\.61|54\.83\.43\.69|52\.69\.166\.231|216\.58\.192\.225|'
                                '118\.184\.25\.86|67\.208\.74\.71|23\.253\.126\.58|104\.239\.157\.210|175\.126\.123\.219|141\.8\.224\.221|10\.10\.10\.10|43\.229\.108\.32|103\.232\.215\.140|69\.172\.201\.153|'
                                '216\.218\.185\.162|54\.225\.104\.146|103\.243\.24\.98|199\.59\.243\.120|31\.170\.160\.61|213\.19\.128\.77|62\.113\.226\.131|208\.100\.26\.234|195\.16\.127\.102|195\.16\.127\.157|'
                                '34\.196\.13\.28|103\.224\.212\.222|172\.217\.4\.225|54\.72\.9\.51|192\.64\.147\.141|198\.200\.56\.183|23\.253\.164\.103|52\.48\.191\.26|52\.214\.197\.72|87\.98\.255\.18|209\.99\.17\.27|'
                                '216\.38\.62\.18|104\.130\.124\.96|47\.89\.58\.141|78\.46\.211\.158|54\.86\.225\.156|54\.82\.156\.19|37\.157\.192\.102|204\.11\.56\.48|110\.34\.231\.42', ip_address)
            if url_match:
                return -1
            elif ip_match:
                return -1
            return 1
        except:
            return 1

    def getFeaturesList(self):
        return self.features

mean_shift_model.set_params(bandwidth=10, bin_seeding=1)
mean_shift_model.fit(X_train,y_train)

url="https://instagram.com/"
#can provide any URL. this URL was taken from PhishTank
obj = FeatureExtraction(url)
x = np.array(obj.getFeaturesList()).reshape(1,30)
y_pred =bagging_model .predict(x)[0]
if y_pred==1:
  print("We guess it is a safe website")
else:
  print("Caution! Suspicious website detected")
